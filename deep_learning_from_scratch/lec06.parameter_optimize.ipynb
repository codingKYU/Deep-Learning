{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 매개변수 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SDG\n",
    "일반적인 매개변수 갱신 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDG:\n",
    "    def __init__(self,lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self,params,grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer = 최적화 기법 (ex. SDG, Momentum ...) 으로 많이 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SDG의 단점 : 비등방성 함수(방향에 따라 성질, 기울기가 달라지는 함수)에서 탐색 경로가 비효율적이다. \n",
    "\n",
    "--> Momentum, AdaGrad, Adam으로 대체 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Momentum\n",
    "\n",
    "속도(v) = a(0.9로 많이 가정) * v(속도) - lr(학습률) * W에 대한 손실함수의 기울기\n",
    "\n",
    "W = W + v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self,lr = 0.01,momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None  #처음에는 어떤 값도 담지 않고 있다.\n",
    "    \n",
    "    def update(self,params,grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}  #아무것도 담지 않고 있는 v값에 딕셔너리를 생성해준다.\n",
    "            for key, val in params.items():  #.items() : keys와 values를 쌍으로 묶어서 보여준다.\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum *self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모멘텀의 갱신 경로 = 공이 그릇 바닥을 구르듯 움직임\n",
    "\n",
    "--> SDG와 비교했을 때 더 안정적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.AdaGrad\n",
    "\n",
    ": 각각의 매개변수에 맞춤형 학습률을 생성.\n",
    "    \n",
    "h(새로운 변수 : 학습률을 조정할 변수) = h + (W에 대한 손실함수의 기울기 * W에 대한 손실함수의 기울기\n",
    "\n",
    "W = W - lr * (1/루트h) * W에 대한 손실함수의 기울기 -> h가 커질수록(W가 많이 움직였을수록) 학습률이 낮게 만든다. 매개변수마다의 학습률이 다르다는 것을 알수있음.\n",
    "\n",
    "상위호환 : RMSProp --> 지수 이동평균을 사용하여 과거 기울기의 반영 규모를 기하급수적으로 감소시킴. AdaGrad의 단점 극복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self,lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self,params,grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.item():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.key():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)  #1e-7을 더해주는 이유 : 만약 h가 0값이여도 계산이 가능하게 만들기 위해."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Adam\n",
    "\n",
    ": Momentum + AdaGrad + 편향 보정\n",
    "\n",
    "--> 이 4가지 기법 중 단연 나은 기법은 없다. 모두 제각각 장단점이 있음. 요즘은 Adam을 많이 사용하기는 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
